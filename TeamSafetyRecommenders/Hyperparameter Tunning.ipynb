{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tunning\n",
    "In this section we will perform hyperparameter tuning on our best chosen models from the model selection notebook.\n",
    "In here we will select the final features that we will be using for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import csv\n",
    "from numpy import nan as NA\n",
    "from datetime import datetime\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from pandas import *\n",
    "import pickle\n",
    "import requests\n",
    "import os\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Opening Pickled Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_y_z.pickle', 'rb') as xyz:\n",
    "    feat_var = pickle.load(xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  feat_var[0]\n",
    "y =  feat_var[1] #y is \"offense group\"\n",
    "z =  feat_var[2] # z is \"ucr-rank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    102462\n",
       "7     82325\n",
       "4     26379\n",
       "5     23033\n",
       "8     22602\n",
       "3     17686\n",
       "2      2170\n",
       "1       942\n",
       "9       196\n",
       "Name: ucr-rank, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_i = pd.Series(z)\n",
    "z_i.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[[\"hour\",\"street\", \"month\", \"day\", \"LATITUDE\", \"LONGITUDE\", \"Temperature\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hyperparameter Tunning with the target variable \"offensegroup\" y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Defining training, test sets and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2,random_state= 0)\n",
    "scaler = StandardScaler()\n",
    "#scaler =  MinMaxScaler()\n",
    "#scaler = Normalizer()\n",
    "X_train = scaler.fit(X_train).transform(X_train)\n",
    "X_test = scaler.fit(X_test).transform(X_test)\n",
    "\n",
    "#Note that there is no scaling in the y set. this is to prevent it to be transformed to continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Defining the tunning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(model, parameters):\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy', cv = 4, n_jobs = -1, verbose = 10)\n",
    "    grid_search = grid_search.fit(X_train, y_train)\n",
    "    accuracy = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    return accuracy, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Selecting the best models for gridsearch with cv of 4 ( to reduce computation time and cpu temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "rnf = RandomForestClassifier()\n",
    "ann = MLPClassifier()\n",
    "sgd = SGDClassifier()\n",
    "baggin = BaggingClassifier()\n",
    "list_params = [{'n_neighbors':[1, 3, 5, 10, 30, 50]}, {'n_estimators' :[50,128,300, 500, 1000]},\n",
    "               {'hidden_layer_sizes': [(100,), (200,), (300,)], 'max_iter': [200, 400, 1000]},\n",
    "              {'alpha': [0.0001, 0.001, 0.1, 1, 10, 50]}, {'n_estimators': [1,10,50,100], 'max_features': [1,3,5],\n",
    "                                                           'max_samples': [1,20,50,100]}]\n",
    "\n",
    "\n",
    "models = [knn, rnf, ann, sgd, baggin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.5min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  2.9min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  4.2min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  5.5min remaining:   46.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  5.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  5.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.8300275382926259], 'best_params': [{'n_neighbors': 50}]}\n",
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   32.3s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:  1.4min remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  3.9min remaining:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  20 | elapsed:  7.3min remaining:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed: 13.8min remaining:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 13.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 13.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.8300275382926259, 0.8317059342320776], 'best_params': [{'n_neighbors': 50}, {'n_estimators': 1000}]}\n",
      "Fitting 4 folds for each of 9 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  36 | elapsed:  4.2min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  36 | elapsed:  5.4min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed:  6.2min remaining:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  6.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.8300275382926259, 0.8317059342320776, 0.8306529995140302], 'best_params': [{'n_neighbors': 50}, {'n_estimators': 1000}, {'hidden_layer_sizes': (200,), 'max_iter': 200}]}\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:    3.5s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:    3.6s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:    4.0s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:    4.1s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    4.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    4.3s finished\n",
      "c:\\users\\franc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.8300275382926259, 0.8317059342320776, 0.8306529995140302, 0.8300185388505913], 'best_params': [{'n_neighbors': 50}, {'n_estimators': 1000}, {'hidden_layer_sizes': (200,), 'max_iter': 200}, {'alpha': 0.0001}]}\n",
      "Fitting 4 folds for each of 48 candidates, totalling 192 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   48.1s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   57.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 out of 192 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.8300275382926259, 0.8317059342320776, 0.8306529995140302, 0.8300185388505913, 0.8300500368977124], 'best_params': [{'n_neighbors': 50}, {'n_estimators': 1000}, {'hidden_layer_sizes': (200,), 'max_iter': 200}, {'alpha': 0.0001}, {'max_features': 5, 'max_samples': 50, 'n_estimators': 100}]}\n"
     ]
    }
   ],
   "source": [
    "scores_dict = {'accuracy': [], 'best_params': []}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for model, param in zip(models, list_params):\n",
    "\n",
    "        acu, best_params = gridsearch(model, param)\n",
    "        scores_dict['accuracy'].append(acu)\n",
    "        scores_dict['best_params'].append(best_params)\n",
    "\n",
    "        print (scores_dict)\n",
    "\n",
    "    with open('gridsearch_y.pickle', 'wb') as f:\n",
    "        pickle.dump(scores_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tunning with the target variable \"ucrrank\" z with 3 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run 0.0 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  feat_var[0]\n",
    "y =  feat_var[1] #y is \"offense group\"\n",
    "z =  feat_var[2] # z is \"ucr-rank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[[\"hour\",\"street\", \"month\", \"day\", \"LATITUDE\", \"LONGITUDE\", \"Temperature\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    102462\n",
       "7     82325\n",
       "4     26379\n",
       "5     23033\n",
       "8     22602\n",
       "3     17686\n",
       "2      2170\n",
       "1       942\n",
       "9       196\n",
       "Name: ucr-rank, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_i = pd.Series(z)\n",
    "z_i.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Defining training, test sets and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_cat(z):\n",
    "    \n",
    "    new_cat = []\n",
    "    for i in z:\n",
    "        if i >= 1 and i < 5:\n",
    "            i = 3\n",
    "        elif i == 7:\n",
    "            i = 2\n",
    "        elif i == 8:\n",
    "            i = 2\n",
    "        else:\n",
    "            i = 1\n",
    "        new_cat.append(i)\n",
    "    z = pd.Series(new_cat)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = new_cat(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "zi = pd.Series(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    125691\n",
       "2    104927\n",
       "3     47177\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zi.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, z_train, z_test = model_selection.train_test_split(X, z, test_size=0.2,random_state= 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#scaler =  MinMaxScaler()\n",
    "#scaler = Normalizer()\n",
    "X_train = scaler.fit(X_train).transform(X_train)\n",
    "X_test = scaler.fit(X_test).transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(model, parameters):\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy', cv = 4, n_jobs = -1, verbose = 10)\n",
    "    grid_search = grid_search.fit(X_train, z_train)\n",
    "    accuracy = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    return accuracy, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:  1.4min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:  2.5min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  3.9min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  5.1min remaining:   43.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  5.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  5.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': [0.5098678881909322], 'best_params': [{'n_neighbors': 50}]}\n",
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  20 | elapsed:  1.6min remaining:  2.4min\n",
      "Exception in thread Thread-42:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\franc\\appdata\\local\\programs\\python\\python36\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\franc\\appdata\\local\\programs\\python\\python36\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\users\\franc\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\pool.py\", line 463, in _handle_results\n",
      "    task = get()\n",
      "  File \"c:\\users\\franc\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"c:\\users\\franc\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\connection.py\", line 318, in _recv_bytes\n",
      "    return self._get_more_data(ov, maxsize)\n",
      "  File \"c:\\users\\franc\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\connection.py\", line 344, in _get_more_data\n",
      "    f.write(ov.getbuffer())\n",
      "MemoryError\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores_dict2 = {'accuracy': [], 'best_params': []}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for model, param in zip(models, list_params):\n",
    "\n",
    "        acu, best_params = gridsearch(model, param)\n",
    "        scores_dict2['accuracy'].append(acu)\n",
    "        scores_dict2['best_params'].append(best_params)\n",
    "\n",
    "        print (scores_dict2)\n",
    "\n",
    "    with open('gridsearch_z3.pickle', 'wb') as g:\n",
    "            pickle.dump(scores_dict2, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tunning with the target variable \"offensegroup\" z with 2 classes(car theft and motor vehicle theft into violent)\n",
    "\n",
    "This Variable Will not be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run 0.0 again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  feat_var[0]\n",
    "y =  feat_var[1] #y is \"offense group\"\n",
    "z =  feat_var[2] # z is \"ucr-rank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_i = pd.Series(z)\n",
    "z_i.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_cat2(z):\n",
    "    new_cat = []\n",
    "    for i in z:\n",
    "        if i >= 1 and i < 5:\n",
    "            i = 2\n",
    "        elif i == 7:\n",
    "            i = 2\n",
    "        elif i == 8:\n",
    "            i = 2\n",
    "        else:\n",
    "            i = 1\n",
    "        new_cat.append(i)\n",
    "    z = pd.Series(new_cat)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = new_cat2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zi = pd.Series(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, z_train, z_test = model_selection.train_test_split(X, z, test_size=0.2,random_state= 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#scaler =  MinMaxScaler()\n",
    "#scaler = Normalizer()\n",
    "X_train = scaler.fit(X_train).transform(X_train)\n",
    "X_test = scaler.fit(X_test).transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(model, parameters):\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'accuracy', cv = 4, n_jobs = -1, verbose = 10)\n",
    "    grid_search = grid_search.fit(X_train, z_train)\n",
    "    accuracy = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    return accuracy, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict3 = {'accuracy': [], 'best_params': []}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for model, param in zip(models, list_params):\n",
    "\n",
    "        acu, best_params = gridsearch(model, param)\n",
    "        scores_dict3['accuracy'].append(acu)\n",
    "        scores_dict3['best_params'].append(best_params)\n",
    "\n",
    "        print (scores_dict3)\n",
    "\n",
    "    with open('gridsearch_z2.pickle', 'wb') as h:\n",
    "            pickle.dump(scores_dict3, h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
